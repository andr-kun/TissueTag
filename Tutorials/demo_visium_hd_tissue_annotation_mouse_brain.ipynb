{
 "cells": [
  {
   "cell_type": "code",
   "id": "aa2e4bca",
   "metadata": {},
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a656df0f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# ðŸ§  TissueTag Annotation Tutorial: Visium HD H&E Image of Mouse Brain\n",
    "\n",
    "Welcome!  \n",
    "In this tutorial, weâ€™ll walk through the process of annotating a **Visium HD H&E image** from a **fresh-frozen mouse brain sample**. This notebook will guide you from raw data download to generating high-quality tissue annotations using both automated and manual approaches.\n",
    "\n",
    "## ðŸ“¦ Download the Sample Data\n",
    "\n",
    "We'll use the publicly available dataset from 10x Genomics.  \n",
    "Run the following commands in your terminal to download all necessary files and set up the demo directory\n",
    "\n",
    "<small>\n",
    "\n",
    "```bash\n",
    "mkdir ../data/tissue_tag_example_visiumHD\n",
    "chdir ../data/tissue_tag_example_visiumHD\n",
    "curl -O https://cf.10xgenomics.com/samples/spatial-exp/3.1.1/Visium_HD_Mouse_Brain_Fresh_Frozen/Visium_HD_Mouse_Brain_Fresh_Frozen_web_summary.html\n",
    "curl -O https://cf.10xgenomics.com/samples/spatial-exp/3.1.1/Visium_HD_Mouse_Brain_Fresh_Frozen/Visium_HD_Mouse_Brain_Fresh_Frozen_cloupe_008um.cloupe\n",
    "curl -O https://cf.10xgenomics.com/samples/spatial-exp/3.1.1/Visium_HD_Mouse_Brain_Fresh_Frozen/Visium_HD_Mouse_Brain_Fresh_Frozen_feature_slice.h5\n",
    "curl -O https://cf.10xgenomics.com/samples/spatial-exp/3.1.1/Visium_HD_Mouse_Brain_Fresh_Frozen/Visium_HD_Mouse_Brain_Fresh_Frozen_metrics_summary.csv\n",
    "curl -O https://cf.10xgenomics.com/samples/spatial-exp/3.1.1/Visium_HD_Mouse_Brain_Fresh_Frozen/Visium_HD_Mouse_Brain_Fresh_Frozen_molecule_info.h5\n",
    "curl -O https://cf.10xgenomics.com/samples/spatial-exp/3.1.1/Visium_HD_Mouse_Brain_Fresh_Frozen/Visium_HD_Mouse_Brain_Fresh_Frozen_spatial.tar.gz\n",
    "curl -O https://cf.10xgenomics.com/samples/spatial-exp/3.1.1/Visium_HD_Mouse_Brain_Fresh_Frozen/Visium_HD_Mouse_Brain_Fresh_Frozen_binned_outputs.tar.gz\n",
    "tar -xvzf Visium_HD_Mouse_Brain_Fresh_Frozen_spatial.tar.gz\n",
    "tar -xvzf Visium_HD_Mouse_Brain_Fresh_Frozen_binned_outputs.tar.gz\n",
    "``` \n",
    "</small>\n",
    "\n",
    "\n",
    "## ðŸ§­ Annotation Strategy\n",
    "\n",
    "Weâ€™ll explore several annotation strategies, ranging from automated predictions to manual refinement:\n",
    "\n",
    "1. **Fully Automatic** â€“ Quickly capture broad tissue features (e.g., tissue boundaries, white and gray matter) using a simple pixel classifier.  \n",
    "2. **Semi-Automatic** â€“ Gene-guided labeling combined with manual annotation and drawing for finer control.\n",
    "\n",
    "All annotations will be saved as a `TissueTag` annotation class object in an `.h5` file.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "Let's get started! ðŸš€\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "recreational-notebook",
   "metadata": {
    "is_executing": true
   },
   "source": [
    "\n",
    "# initialisation \n",
    "import os\n",
    "import panel as pn\n",
    "import bin2cell as b2c\n",
    "import tissue_tag as tt\n",
    "import tissue_tag.annotation\n",
    "import tissue_tag.io\n",
    "from tissue_tag.io import TissueTagAnnotation\n",
    "os.environ[\"BOKEH_ALLOW_WS_ORIGIN\"] = \"*\"\n",
    "# host = <port> # set the port to the value in the address bar when operating in farm\n",
    "host = '8888' # when working locally e.g. desktop"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "bcf3da29-23f2-4bb8-92e1-72eaa9f81a53",
   "metadata": {},
   "source": [
    "# ðŸš€ Initiate\n",
    "\n",
    "We will start by:\n",
    "\n",
    "- Reading in the image data  \n",
    "- Defining the resolution weâ€™ll work with  \n",
    "- Loading the associated `AnnData` object  \n",
    "\n",
    "> âš ï¸ **Note:** `TissueTag` is designed for **tissue-level** annotations. We do **not** recommend using it for annotating very small structures, such as single cells.  \n",
    "> For best performance and clarity, we recommend working at a resolution of **0.5 pixels per micron (ppm)** â€” equivalent to **2 microns per pixel**.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "d5b3fee813d95d01",
   "metadata": {},
   "source": [
    "# set paths\n",
    "# Here we will use the 16um binned data that is available from 10X for the annotation, don't worry, later you can map these annotations to any other resolution (2um, 8um and even a single cell Bin2Cell object - https://github.com/Teichlab/bin2cell )\n",
    "spaceranger_dir_path = \"../data/tissue_tag_example_visiumHD\"\n",
    "mapped_image_path = spaceranger_dir_path + '/Visium_HD_Mouse_Brain_Fresh_Frozen_tissue_image.tif' # the image that was used for spaceranger input\n",
    "spaceranger_spatial_path = spaceranger_dir_path +'/spatial' # path to the spatial folder for this dataset (this structure was changed a few times) \n",
    "bin_resolution = 16 # bin resolution to work with\n",
    "\n",
    "tt_obj = tissue_tag.io.read_visium_hd(\n",
    "    spaceranger_dir_path=spaceranger_dir_path, \n",
    "    mapped_image_path=mapped_image_path,\n",
    "    use_resolution='mapped_res', # use the max resolution \n",
    "    ppm_out=0.5,\n",
    "    plot=True,\n",
    "    bin_resolution=bin_resolution)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b103bb297616006b",
   "metadata": {},
   "source": [
    "# Automatic annotations - Create de-novo annotations from gene expression (or not)"
   ]
  },
  {
   "cell_type": "code",
   "id": "77263247",
   "metadata": {},
   "source": [
    "# read anndata with bin2cell make sure you are using the same binning resolution!!\n",
    "adata = b2c.read_visium(\n",
    "    path = spaceranger_dir_path + f'/binned_outputs/square_0{bin_resolution}um/',\n",
    "    spaceranger_image_path = spaceranger_spatial_path\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c6d2db341c106808",
   "metadata": {},
   "source": [
    "# Define anatomical structure annotations\n",
    "# and their associated color codes.\n",
    "#\n",
    "# Color families:\n",
    "#   â€¢ Red:         'red', 'darkred', 'firebrick', 'indianred'\n",
    "#   â€¢ Green:       'green', 'darkgreen', 'lime', 'seagreen', 'forestgreen'\n",
    "#   â€¢ Blue:        'blue', 'darkblue', 'royalblue', 'dodgerblue', 'deepskyblue'\n",
    "#   â€¢ Cyan:        'cyan', 'lightcyan', 'darkcyan', 'teal'\n",
    "#   â€¢ Magenta:     'magenta', 'purple', 'darkmagenta', 'orchid', 'violet'\n",
    "#   â€¢ Yellow/Orange:'gold', 'orange', 'darkorange', 'goldenrod'\n",
    "#   â€¢ Brown:       'brown', 'saddlebrown', 'chocolate', 'peru', 'tan'\n",
    "#   â€¢ Gray/Black:  'black', 'gray', 'darkgray', 'dimgray', 'lightgray'\n",
    "#   â€¢ White:       'white'\n",
    "\n",
    "tt_obj.annotation_map = {\n",
    "    'unassigned':      'yellow',\n",
    "    'isocortex':       'green',\n",
    "    'hippocampus':     'darkgreen',\n",
    "    'olfactory':       'orange',\n",
    "    'striatum':        'red',\n",
    "    'thalamus':        'blue',\n",
    "    'amygdala':        'lime',\n",
    "    'choroid_plexus':  'gold',\n",
    "    'pia':             'deepskyblue',\n",
    "    'white_matter':    'white',\n",
    "    'gray_matter':     'teal',\n",
    "    'dentate_gyrus':   'violet',\n",
    "    'layer_1':         'tan',\n",
    "\n",
    "}\n",
    "# note if you need to add annotation to an exxisting object add them in the end and do not change the order as this corresponds to the pixel values of the label image. e.g. unassigned = 1, isocortex=2 etc"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b11345e427911fe",
   "metadata": {},
   "source": [
    "\n",
    "# Define gene markers per region\n",
    "# Format: region_name: List of (gene, expression threshold)\n",
    "\n",
    "gene_markers = {\n",
    "    'gray_matter': [\n",
    "        ('Gad1', 2500),\n",
    "        ('Gad2', 2500),\n",
    "        ('Slc17a7', 3000)\n",
    "    ],\n",
    "    'white_matter': [\n",
    "        ('Mbp', 1500),\n",
    "        ('Gfap',500)\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Generate training labels from gene expression\n",
    "# This maps regions based on marker expression\n",
    "\n",
    "tissue_tag.annotation.gene_labels_from_adata(\n",
    "    adata=adata,\n",
    "    gene_markers=gene_markers,\n",
    "    tissue_tag_annotation=tt_obj,\n",
    "    diameter=bin_resolution * 4,  # Labeling diameter\n",
    "    override_labels=True,         # Replace any existing labels\n",
    "    normalize=True               # Use raw expression\n",
    ")\n",
    "\n",
    "\n",
    "# Visualize the assigned labels\n",
    "tissue_tag.annotation.median_filter(tt_obj,filter_radius=bin_resolution)\n",
    "tissue_tag.annotation.plot_labels(tt_obj, alpha=0.5) # i wasn't able to supress putput in annotation.plot_labels\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1e14db2f",
   "metadata": {},
   "source": [
    "\n",
    "### ðŸ§  Pixel Classification\n",
    "\n",
    "In this step, we train a **Random Forest pixel classifier** using the labeled regions provided earlier.  \n",
    "This model will then be used to predict annotations for all image pixels.\n",
    "\n",
    "â±ï¸ **Estimated runtime:** 1â€“10 minutes, depending on image resolution, sysyem performance and the number of training areas.\n",
    "\n",
    "The result will be stored in the existing `TissueTag` object.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "7b7c1c96",
   "metadata": {},
   "source": [
    "%%time\n",
    "tissue_tag.annotation.pixel_label_classifier(tt_obj)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8057fe45",
   "metadata": {},
   "source": [
    "# ðŸ§¬ Gene-Guided Labeling\n",
    "\n",
    "In this step, we overlay **gene expressionâ€“based labels** on top of the previous classification.  \n",
    "These labels serve as additional guidance for **manual tissue annotation**, helping refine regional identity using known gene markers.\n",
    "\n",
    "We define a dictionary of **marker genes** and their expression thresholds for key brain regions.  \n",
    "From this, we generate spatial labels directly from the `AnnData` object and add them to the existing `TissueTag` annotation.\n",
    "\n",
    "> ðŸ” These gene-based labels are not final annotationsâ€”they are **hints** to support accurate manual refinement.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "061d9624",
   "metadata": {},
   "source": [
    "# select gene markers \n",
    "gene_markers = {\n",
    "    'isocortex': [('Pak7',500),('Myl4',500),('Ttc9b',500)],\n",
    "    'amygdala':[('Acvr2a',300)],\n",
    "    'olfactory': [('Cdhr1',500)],\n",
    "    'striatum': [('Adora2a',200),('Gprin3',200)],\n",
    "    'thalamus': [('Plekhg1',500)],\n",
    "\t'choroid_plexus':[('Tcf21',500)],\n",
    "    'hippocampus': [('Zbtb20',500)],\n",
    "}\n",
    "\n",
    "# generate training data from gene expression\n",
    "tissue_tag.annotation.gene_labels_from_adata(\n",
    "    adata = adata,\n",
    "    gene_markers = gene_markers,    \n",
    "    tissue_tag_annotation=tt_obj,\n",
    "    diameter = bin_resolution*2,\n",
    "    normalize=False,\n",
    "\n",
    ") # generate gene-marker-labels\n",
    "tissue_tag.annotation.plot_labels(tt_obj, alpha=0.25)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "afb78cf7",
   "metadata": {},
   "source": [
    "### âœï¸ Interactive Tissue Annotator\n",
    "\n",
    "Now it's time to manually refine the tissue annotation using the **interactive annotator**, powered by **Panel** and **Datashader**.\n",
    "\n",
    "This tool gives you full control over the labeling process with an intuitive interface:\n",
    "\n",
    "1. ðŸ–¼ï¸ **Image Toggle**  \n",
    "   Use the slider to toggle between the raw image and the current annotation labels for visual comparison.\n",
    "\n",
    "2. ðŸŽ¨ **Structure Selector**  \n",
    "   On the left, you'll see each tissue structure represented by its **first letter** in color.  \n",
    "   Hover over a letter to reveal the **full region name**.\n",
    "\n",
    "3. ðŸ–‹ï¸ **Drawing Annotations**  \n",
    "   Select a tissue structure and **draw freely** over the region of interest.  \n",
    "   Then, click the **Update** button to \"fill\" the sketched shapes into annotations.  \n",
    "   *(Shapes do not need to be closed manuallyâ€”this is handled for you.)*\n",
    "\n",
    "4. ðŸ—‘ï¸ **Removing Annotations**  \n",
    "   - If you havenâ€™t pressed **Update** yet, simply click the drawn line and hit `Backspace`.  \n",
    "   - If you've already updated, you can click **Revert** to undo the last annotation fill.\n",
    "\n",
    "---\n",
    "\n",
    "> âš™ï¸ **Rendering Option: `use_datashader=True`**  \n",
    "> At this stage, you can choose whether to render the image using **Datashader**.  \n",
    "> Setting `use_datashader=True` is **recommended for large images or high-resolution annotations**.  \n",
    "> While annotation drawing is slightly slower with Datashader, it ensures smooth and responsive image loading.  \n",
    "> Without Datashader, very large images might fail to load or take a very long time to render.\n",
    "\n",
    "---\n",
    "\n",
    "> âš ï¸ **Tip:** For the best experience, we **highly recommend using a mouse with a scroll wheel**. Trackpads or touchscreen input may behave inconsistently.\n",
    "\n",
    "All annotations are automatically assigned to your `TissueTag` annotation object and can be saved later as part of the `.h5` file.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "80ab0da6",
   "metadata": {},
   "source": [
    "# use annotator to label tissue regions according to categories indicated above\n",
    "annotator = tissue_tag.annotation.annotator(tt_obj, use_datashader=True)\n",
    "pn.io.notebook.show_server(annotator, notebook_url=f'localhost:'+host)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b70b63b6",
   "metadata": {},
   "source": [
    "tissue_tag.annotation.plot_labels(tt_obj, alpha=0.5)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e1b98a65",
   "metadata": {},
   "source": [
    "# ðŸ’¾ Save Annotation Output\n",
    "\n",
    "After completing the annotation process, we save the results for future use.  \n",
    "This includes the labeled image, annotation map, and any interactive edits made during the session.\n",
    "\n",
    "Annotations will be saved as an `.h5` file inside the `tt_annotations` folder, within the binned output directory:\n",
    "\n",
    "- The output path is based on the current bin resolution (e.g., `square_16um` for 16Î¼m bins).\n",
    "- If the target directory does not exist, it will be created automatically.\n",
    "\n",
    "This allows seamless integration with downstream analysis or loading annotations in a future session using `TissueTag`.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "b998991c",
   "metadata": {},
   "source": [
    "path = spaceranger_dir_path + f'/binned_outputs/square_0{bin_resolution}um/'\n",
    "isExist = os.path.exists(path+'tt_annotations')\n",
    "if not(isExist):\n",
    "    os.mkdir(path+'/tt_annotations/')\n",
    "    \n",
    "tt_obj.save_annotation(file_path=path+'/tt_annotations/annotations_v1.h5')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2085d02d",
   "metadata": {},
   "source": [
    "# we recommend loading the object to make sure it's all saved properly \n",
    "tt_obj = tt.load_annotation(file_path=path+'/tt_annotations/annotations_v1.h5')\n",
    "im = tissue_tag.annotation.plot_labels(tt_obj, alpha=0.5)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8f0df8a1",
   "metadata": {},
   "source": [
    "# ðŸ“ Calculate Spatial Distances to Annotations\n",
    "\n",
    "In this step, we generate a **regular spatial grid** across the annotated image and compute the **mean distance from each grid point to nearby annotated tissue structures**.\n",
    "\n",
    "This allows us to:\n",
    "\n",
    "- Consistently quantify spatial proximity to anatomical regions\n",
    "- Create interpretable spatial axes (e.g. cortical gradients, medial-lateral transitions)\n",
    "\n",
    "#### ðŸ§­ How it works:\n",
    "\n",
    "1. **Grid Generation**  \n",
    "   A tight hexagonal grid is generated across the tissue using `grid_unit_size` spacing (in microns).  \n",
    "   Each grid point is assigned a label based on the median annotation value \"under\" each grid spot.\n",
    "\n",
    "2. **Neighborhood Distance Calculation**  \n",
    "   Using a fast KD-tree implementation, the grid is queried against each annotated region.  \n",
    "   For each annotated structure, we calculate the **mean distance** to its `k` nearest neighbors (`nhood_size`), for every grid point.\n",
    "\n",
    "---\n",
    "\n",
    "> âš™ï¸ **Parameters Used**  \n",
    "> - `grid_unit_size = 15`: spacing between grid points, in microns  \n",
    "> - `nhood_size = 10`: number of nearest neighbors used for computing distances  \n",
    "\n",
    "---\n",
    "\n",
    "The output is stored in `tt_obj.grid`, containing:\n",
    "\n",
    "- Coordinates (`x`, `y`)\n",
    "- Region labels and numeric codes\n",
    "- Per-category distance columns (e.g. `L2_dist_annotation_hippocampus`)\n",
    "\n",
    "You can use this grid for further downstream tasks such as:\n",
    "\n",
    "- Visualizing spatial trends\n",
    "- Transferring annotations to data (`map_annotations_to_target`) e.g. 16/8/2um/Bin2Cell object from the same visium HD dataset\n",
    "- Creating spatial axes (with `calculate_axis_2p/3p`)\n",
    "- Binning space into anatomical domains (`bin_axis`)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "95dd6ddf",
   "metadata": {},
   "source": [
    "# tt_obj = tt.load_annotation(file_path=path+'/tt_annotations/annotations.h5')\n",
    "grid_unit_size = 15\n",
    "nhood_size = 10\n",
    "\n",
    "########## this part should be merged into a future function tt_obj.grid = calculate_distances_to_nhood(tt_obj,grid_unit_size,nhood_size)\n",
    "tt_obj.grid = tt.generate_grid_from_annotation(tt_obj, spot_to_spot = grid_unit_size)\n",
    "print('calculating distances')\n",
    "tt.calculate_distance_to_annotations(\n",
    "    tt_obj.grid,\n",
    "    knn=nhood_size\n",
    "    ) # calculate minimum mean distance of each spot to clusters\n",
    "tt_obj.grid['annotation'].value_counts()\n",
    "\n",
    "##########\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "da239da3",
   "metadata": {},
   "source": [
    "### ðŸ”„ Map Grid Annotations to Visium HD 16Î¼m Spots\n",
    "\n",
    "Now that weâ€™ve generated the annotated spatial grid, we map these annotations back to the original **Visium HD 16Î¼m object** to enable downstream integration with gene expression data.\n",
    "\n",
    "#### ðŸ§­ Step Overview:\n",
    "\n",
    "1. **Extract Spot Coordinates**  \n",
    "   We directly convert `adata.obsm['spatial']` into a DataFrame with `\"x\"` and `\"y\"` columns, using the spot barcodes from `adata.obs_names` as the index.\n",
    "\n",
    "2. **Calculate Pixels-Per-Micron (ppm)**  \n",
    "   The spatial scaling is retrieved from `adata.uns['spatial']`, and the pixels-per-micron (`ppm_target`) is computed as the inverse of `\"microns_per_pixel\"`.\n",
    "\n",
    "3. **Map Annotations**  \n",
    "   We use `tt.map_annotations_to_target()` to assign annotations from the grid to Visium spots using a **KD-tree nearest-neighbor search**.  \n",
    "   The `max_distance` parameter ensures that only nearby annotations (within 2Ã— spot size) are transferred.\n",
    "\n",
    "4. **Alignment Preview**  \n",
    "   A scatter plot is automatically generated to visually compare the source grid and target Visium spot positions, ensuring spatial alignment.\n",
    "\n",
    "5. **Map to AnnData obs**  \n",
    "   matching back indices and transfer annotations to object \n",
    "\n",
    "---\n",
    "\n",
    "> ðŸ§ª This step is essential to bridge spatial gene expression data with your refined anatomical annotation grid.\n",
    "\n",
    "> ðŸ“Œ `ppm_source` is set to 1 (since the grid is already in pixel space), while_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "e389b607",
   "metadata": {},
   "source": [
    "# Display the resulting DataFrame\n",
    "import pandas as pd\n",
    "tt_obj.grid = tt_obj.grid[tt_obj.grid['annotation']!='unassigned']\n",
    "\n",
    "ppm_target = 1/adata.uns['spatial']['Visium_HD_Mouse_Brain_Fresh_Frozen']['scalefactors']['microns_per_pixel']\n",
    "vis_df = tt.map_annotations_to_target(    \n",
    "    df_target=pd.DataFrame(adata.obsm['spatial'], index=adata.obs_names, columns=[\"x\", \"y\"]),\n",
    "    df_source=tt_obj.grid,\n",
    "    ppm_source=1, # this is always 1\n",
    "    ppm_target=ppm_target, # do one over for ppm,\n",
    "    plot=True,\n",
    "    max_distance=int(grid_unit_size*2/ppm_target)\n",
    ")      \n",
    "\n",
    "# Ensure both DataFrames have the same index type\n",
    "vis_df = vis_df.loc[adata.obs.index]  # Align indices to avoid mismatches\n",
    "\n",
    "# Add columns from the fourth onward to adata_vis.obs\n",
    "adata.obs = pd.concat([adata.obs, vis_df.iloc[:, 2:]], axis=1)\n",
    "adata.obs = adata.obs.loc[:, ~adata.obs.columns.duplicated()]\n",
    "# Display the updated adata_vis.obs\n",
    "adata.obs"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "0928908d",
   "metadata": {},
   "source": [
    "# ðŸ§­ Compute Spatial Axes Between Anatomical Landmarks\n",
    "\n",
    "To better capture spatial organization within complex brain regions, we calculate **continuous spatial axes** that represent each spotâ€™s position **relative to two anatomical landmarks**.\n",
    "\n",
    "#### ðŸ“ Axis Calculation\n",
    "\n",
    "Using the `tt.calculate_axis_2p()` function, we generate **directional, normalized spatial gradients** between selected anatomical regions:\n",
    "\n",
    "- `pia_to_wm`: Measures depth along the **cortical column**, from the pia surface to underlying white matter.\n",
    "- `olf_to_wm`: Captures positioning along the **olfactoryâ€“white matter axis**, useful for modeling transitions in **striatal** regions.\n",
    "- `iso_to_wm`: Represents spatial location **within the isocortex**, from the cortical-corpus callosum interface as a 0 point.\n",
    "- `hip_to_den`: Describes a spatial axis within the **hippocampus**, spanning from hippocampal tissue toward the **dentate gyrus**.  \n",
    "  *(This one is exploratory and not grounded in known biological gradientsâ€”just for fun!) ðŸ˜‰\n",
    "\n",
    "Each computed axis is added as a new column in `adata.obs`, enabling further stratification, visualization, and spatial modeling.\n",
    "\n",
    "\n",
    "#### ðŸŽ¯ Contextual Filtering\n",
    "\n",
    "To improve specificity and interpretability, we apply **region-aware masking** to restrict each spatial axis to its relevant anatomical context:\n",
    "\n",
    "- `pia_to_wm` is retained only in **isocortex** or **layer_1**\n",
    "- Spots located far from both **pia** and **white matter** (> ~1000 microns\\*) are excluded\n",
    "- `olf_to_wm` is restricted to the **striatum**\n",
    "- `iso_to_wm` is limited to **isocortex** and **white matter**, and further filtered to spots within ~300 microns\\* of either structure\n",
    "- `hip_to_den` is retained only in **hippocampus** or **dentate gyrus**\n",
    "\n",
    "\\* _Note:_ These distances are calculated from the **mean distance to a set of nearby points (neighborhood)**, not from a single closest spot.  \n",
    "If you need distances that reflect the **exact distance to the single nearest spot**, set `nhood_size=1` when computing distances.  \n",
    "However, keep in mind that setting `nhood_size=1` may limit your ability to calculate smooth spatial axes **within structures**â€”as done in `iso_to_wm` and `hip_to_den`.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "e39d1610",
   "metadata": {},
   "source": [
    "\n",
    "tt.calculate_axis_2p(adata.obs, structure = ['pia','white_matter'], output_col='pia_to_wm')\n",
    "tt.calculate_axis_2p(adata.obs, structure = ['olfactory','white_matter'], output_col='olf_to_wm')\n",
    "tt.calculate_axis_2p(adata.obs, structure = ['isocortex','white_matter'], output_col='iso_to_wm')\n",
    "tt.calculate_axis_2p(adata.obs, structure = ['hippocampus','dentate_gyrus'], output_col='hip_to_den')\n",
    "\n",
    "\n",
    "# here we will do \n",
    "adata.obs.loc[~adata.obs['annotation'].isin(['isocortex','layer_1']),'pia_to_wm'] = None\n",
    "\n",
    "mask = (\n",
    "    (adata.obs['L2_dist_annotation_white_matter'] > 1000) &\n",
    "    (adata.obs['L2_dist_annotation_pia'] > 1000)\n",
    ")\n",
    "adata.obs.loc[mask, 'pia_to_wm'] = None\n",
    "adata.obs.loc[~adata.obs['annotation'].isin(['striatum']),'olf_to_wm'] = None\n",
    "adata.obs.loc[~adata.obs['annotation'].isin(['isocortex','white_matter']),'iso_to_wm'] = None\n",
    "adata.obs.loc[adata.obs['L2_dist_annotation_white_matter']>300,'iso_to_wm'] = None\n",
    "adata.obs.loc[adata.obs['L2_dist_annotation_isocortex']>300,'iso_to_wm'] = None\n",
    "\n",
    "adata.obs.loc[~adata.obs['annotation'].isin(['hippocampus','dentate_gyrus']),'hip_to_den'] = None\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0c0c6c97",
   "metadata": {},
   "source": [
    "# plot the newly annotated visium AnnData\n",
    "import scanpy as sc\n",
    "sc.set_figure_params(figsize=[5,5],dpi=100)\n",
    "sc.pl.spatial(adata,color=['pia_to_wm','olf_to_wm','hip_to_den','iso_to_wm'],cmap='gist_rainbow',ncols=2)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5ed17047",
   "metadata": {},
   "source": [
    "\n",
    "# Get all obs columns that contain \"L2_dist\"\n",
    "l2_dist_cols = [col for col in adata.obs.columns if 'L2_dist' in col]\n",
    "\n",
    "# Plot them using scanpy\n",
    "sc.set_figure_params(figsize=[5, 5], dpi=100)\n",
    "sc.pl.spatial(adata, color=l2_dist_cols, cmap='jet', ncols=5)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3a7d6fec",
   "metadata": {},
   "source": [
    "sc.pl.spatial(adata,color=['annotation'],ncols=3) # andrian march the colors!"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "94975b3b",
   "metadata": {},
   "source": [
    "# downstream analysis \n",
    "sc.pp.filter_cells(adata,min_counts=100)\n",
    "sc.pp.normalize_total(adata)\n",
    "sc.pp.log1p(adata)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f4f86fdd",
   "metadata": {},
   "source": [
    "adata.var_names_make_unique()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "967f2aec",
   "metadata": {},
   "source": [
    "sc.pp.highly_variable_genes(adata,n_top_genes=5000)\n",
    "adata = adata[:, adata.var[\"highly_variable\"]].copy()\n",
    "sc.pp.scale(adata, max_value=10)\n",
    "sc.pp.pca(adata, use_highly_variable=True)\n",
    "sc.pp.neighbors(adata)\n",
    "sc.tl.umap(adata)\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "eba46edf",
   "metadata": {},
   "source": [
    "sc.pl.umap(adata,color=['pia_to_wm','olf_to_wm','hip_to_den','iso_to_wm'],cmap='gist_rainbow',ncols=2,s=2)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "64324215",
   "metadata": {},
   "source": [
    "sc.pl.umap(adata,color=['annotation'],ncols=2,s=2)"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
